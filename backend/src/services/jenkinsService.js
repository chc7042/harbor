const axios = require('axios');
const logger = require('../config/logger');
const { withJenkinsRetry, withNASRetry } = require('../utils/retryMechanism');
const { getDeploymentPathService } = require('./deploymentPathService');
const { getNASService } = require('./nasService');
const { getMetricsService } = require('./metricsService');
const { getAlertingService } = require('./alertingService');
const {
  formatDateForNAS,
  generatePathCandidates,
  constructNASPath,
  determineMainDownloadFile,
  categorizeFiles,
} = require('../utils/pathDetection');

class JenkinsService {
  constructor() {
    this.baseURL = process.env.JENKINS_URL;
    this.username = process.env.JENKINS_USERNAME;
    this.password = process.env.JENKINS_PASSWORD;

    // Jenkins ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì—ëŸ¬
    if (!this.baseURL || !this.username || !this.password) {
      throw new Error('Jenkins configuration is required: JENKINS_URL, JENKINS_USERNAME, JENKINS_PASSWORD must be set');
    }
    this.auth = Buffer.from(`${this.username}:${this.password}`).toString('base64');

    this.client = axios.create({
      baseURL: this.baseURL,
      timeout: 30000,
      headers: {
        'Authorization': `Basic ${this.auth}`,
        'Content-Type': 'application/json',
        'Accept': 'application/json',
      },
      httpsAgent: new (require('https').Agent)({
        rejectUnauthorized: false,
      }),
    });

    this.client.interceptors.response.use(
      response => response,
      error => {
        logger.error('Jenkins API Error:', {
          url: error.config?.url,
          status: error.response?.status,
          message: error.message,
        });
        throw error;
      },
    );
  }

  async getJobs() {
    return withJenkinsRetry(async () => {
      // projects í´ë”ì˜ í•˜ìœ„ í´ë”ë“¤ ì¡°íšŒ
      const url = '/job/projects/api/json?tree=jobs[name,url]';
      logger.info(`ğŸ”§ Jenkins: Fetching project folders from URL: ${url}`);

      const response = await this.client.get(url);
      const projectFolders = response.data.jobs || [];

      logger.info(`ğŸ”§ Jenkins: Found ${projectFolders.length} project folders:`, projectFolders.map(folder => folder.name));

      // ê° í”„ë¡œì íŠ¸ í´ë”ì—ì„œ ì‹¤ì œ ì‘ì—…ë“¤ ì¡°íšŒ
      const allJobs = [];
      for (const folder of projectFolders) {
        try {
          const folderUrl = `/job/projects/job/${encodeURIComponent(folder.name)}/api/json?tree=jobs[name,url,buildable,lastBuild[number,url,result,timestamp,duration,displayName,actions[parameters[name,value],causes[shortDescription],lastBuiltRevision[branch[name]]]]]`;
          logger.info(`ğŸ”§ Jenkins: Fetching jobs from folder ${folder.name}: ${folderUrl}`);
          const folderResponse = await this.client.get(folderUrl);
          const folderJobs = folderResponse.data.jobs || [];
          logger.info(`ğŸ”§ Jenkins: Found ${folderJobs.length} jobs in folder ${folder.name}:`, folderJobs.map(job => job.name));

          // í•„í„°ë§ ë¡œì§ ê°œì„ : ì¼ë°˜ ë²„ì „ í”„ë¡œì íŠ¸(x.x.x)ì™€ mr/fs í”„ë¡œì íŠ¸ ëª¨ë‘ í¬í•¨
          const filteredJobs = folderJobs.filter(job => {
            const jobName = job.name.toLowerCase();
            const folderName = folder.name.toLowerCase();

            logger.info(`ğŸ”§ Jenkins: Checking job ${jobName} in folder ${folderName}`);

            // ì¼ë°˜ ë²„ì „ í”„ë¡œì íŠ¸ (1.2.0, 2.0.0, 3.0.0, 4.0.0 ë“±)
            const versionPattern = /^\d+\.\d+\.\d+$/;
            if (versionPattern.test(folderName)) {
              // ì¼ë°˜ ë²„ì „ í”„ë¡œì íŠ¸ì˜ ëª¨ë“  ì‘ì—… í¬í•¨
              logger.info(`ğŸ”§ Jenkins: Including job ${jobName} from version folder ${folderName}`);
              return true;
            }

            // mr/fs ë²„ì „ í´ë” (mr1.1.0, fs2.0.0 ë“±)
            const mrFsVersionPattern = /^(mr|fs)\d+\.\d+\.\d+$/;
            if (mrFsVersionPattern.test(folderName)) {
              // mr/fs ë²„ì „ í´ë”ì˜ release ì‘ì—…ë“¤ë§Œ í¬í•¨
              const releasePattern = /^(mr|fs)\d+\.\d+\.\d+_release$/;
              const isReleaseJob = releasePattern.test(jobName);
              if (isReleaseJob) {
                logger.info(`ğŸ”§ Jenkins: Including release job ${jobName} from mr/fs version folder ${folderName}`);
              } else {
                logger.info(`ğŸ”§ Jenkins: Excluding job ${jobName} from mr/fs version folder ${folderName} (not release job)`);
              }
              return isReleaseJob;
            }

            // ê¸°íƒ€ í´ë”ëŠ” ì œì™¸
            logger.info(`ğŸ”§ Jenkins: Excluding job ${jobName} from folder ${folderName} (not matching any pattern)`);
            return false;
          });
          
          logger.info(`ğŸ”§ Jenkins: After filtering, ${filteredJobs.length} jobs included from folder ${folder.name}`);

          // í”„ë¡œì íŠ¸ í´ë” ì´ë¦„ì„ ê° ì‘ì—…ì— ì¶”ê°€í•˜ê³  ë¸Œëœì¹˜ ì •ë³´ ì¶”ì¶œ
          filteredJobs.forEach(job => {
            job.projectFolder = folder.name;
            job.fullJobName = `${folder.name}/${job.name}`;

            // ë¸Œëœì¹˜ ì •ë³´ ì¶”ì¶œ
            if (job.lastBuild && job.lastBuild.actions) {
              job.lastBuild.branch = this.extractBranchInfo(job.lastBuild.actions);
            }
          });

          allJobs.push(...filteredJobs);
          logger.info(`Found ${filteredJobs.length} release jobs (mr/fs x.x.x_release format) out of ${folderJobs.length} total jobs in ${folder.name} folder`);
        } catch (folderError) {
          logger.warn(`Failed to fetch jobs from folder ${folder.name}:`, folderError.message);
        }
      }

      logger.info(`ğŸ”§ Jenkins: Total jobs retrieved: ${allJobs.length}`);
      logger.info(`ğŸ”§ Jenkins: Final job list:`, allJobs.map(job => `${job.projectFolder}/${job.name}`));
      return allJobs;
    }, {}, 'Jenkins getJobs');
  }

  async getAllJobs() {
    try {
      // ì „ì²´ Jenkins ì‘ì—… ëª©ë¡ ì¡°íšŒ (ë£¨íŠ¸ ë ˆë²¨) - ë¸Œëœì¹˜ ì •ë³´ í¬í•¨
      const url = '/api/json?tree=jobs[name,url,buildable,lastBuild[number,url,result,timestamp,duration,displayName,actions[parameters[name,value],causes[shortDescription],lastBuiltRevision[branch[name]]]]]';
      logger.debug(`Fetching all Jenkins jobs from URL: ${url}`);

      const response = await this.client.get(url);
      const rootJobs = response.data.jobs || [];

      logger.info(`Found ${rootJobs.length} root level jobs`);

      // í´ë”ì¸ì§€ ì¼ë°˜ ì‘ì—…ì¸ì§€ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
      const allJobs = [];

      // ì¬ê·€ì ìœ¼ë¡œ í´ë” êµ¬ì¡°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
      const processFolderRecursively = async (parentPath, folderName, depth = 0) => {
        if (depth > 5) { // ë¬´í•œ ì¬ê·€ ë°©ì§€
          logger.warn(`Maximum depth reached for folder: ${parentPath}/${folderName}`);
          return [];
        }

        try {
          const fullPath = parentPath ? `${parentPath}/job/${encodeURIComponent(folderName)}` : `/job/${encodeURIComponent(folderName)}`;
          const folderUrl = `${fullPath}/api/json?tree=jobs[name,url,buildable,lastBuild[number,url,result,timestamp,duration,displayName,actions[parameters[name,value],causes[shortDescription],lastBuiltRevision[branch[name]]]]]`;

          logger.debug(`Processing folder at depth ${depth}: ${folderName}, URL: ${folderUrl}`);

          const folderResponse = await this.client.get(folderUrl);
          const folderJobs = folderResponse.data.jobs || [];
          const jobs = [];

          for (const subJob of folderJobs) {
            // í•˜ìœ„ í´ë”ì¸ ê²½ìš° ì¬ê·€ ì²˜ë¦¬
            if (!subJob.buildable && subJob.url && subJob.url.includes('/job/')) {
              logger.debug(`Found nested folder: ${subJob.name} in ${folderName}`);
              const nestedJobs = await processFolderRecursively(fullPath, subJob.name, depth + 1);
              jobs.push(...nestedJobs);
            } else {
              // ì‹¤ì œ ì‘ì—…ì¸ ê²½ìš°
              if (subJob.lastBuild && subJob.lastBuild.actions) {
                subJob.lastBuild.branch = this.extractBranchInfo(subJob.lastBuild.actions);
              }

              const fullJobPath = parentPath ? `${parentPath.replace('/job/', '')}/${folderName}/${subJob.name}` : `${folderName}/${subJob.name}`;

              jobs.push({
                ...subJob,
                projectFolder: parentPath ? `${parentPath.replace('/job/', '')}/${folderName}` : folderName,
                fullJobName: fullJobPath,
                folderPath: parentPath ? `${parentPath.replace('/job/', '')}/${folderName}` : folderName,
              });
            }
          }

          logger.info(`Found ${jobs.length} jobs in folder ${folderName} at depth ${depth}`);
          return jobs;
        } catch (error) {
          logger.warn(`Failed to process folder ${folderName} at depth ${depth}:`, error.message);
          return [];
        }
      };

      for (const job of rootJobs) {
        try {
          // í´ë”ì¸ ê²½ìš° (buildableì´ falseì´ê³  urlì— jobì´ í¬í•¨ëœ ê²½ìš°)
          if (!job.buildable && job.url && job.url.includes('/job/')) {
            logger.debug(`Processing root folder: ${job.name}`);
            const folderJobs = await processFolderRecursively('', job.name, 0);
            allJobs.push(...folderJobs);
          } else {
            // ì¼ë°˜ ì‘ì—…ì¸ ê²½ìš°
            allJobs.push({
              ...job,
              projectFolder: 'root',
              fullJobName: job.name,
            });
          }
        } catch (jobError) {
          logger.warn(`Failed to process job ${job.name}:`, jobError.message);
        }
      }

      logger.info(`Total jobs found: ${allJobs.length}`);
      return allJobs;
    } catch (error) {
      logger.error('Failed to fetch all Jenkins jobs:', error.message);
      logger.error(`Error status: ${error.response?.status}, URL: /api/json`);
      throw new Error('Jenkins ì „ì²´ ì‘ì—… ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  }

  async getJobBuilds(jobName, limit = 20) {
    try {
      // jobNameì´ ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŒ (ì˜ˆ: "projects/3.0.0/mr/mr3.0.0_release")
      let folderPath, actualJobName;

      if (jobName.includes('/')) {
        const parts = jobName.split('/');
        actualJobName = parts.pop(); // ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ì‹¤ì œ ì‘ì—… ì´ë¦„
        folderPath = parts.join('/');
      } else {
        // ê¸°ì¡´ jobNameì´ ë‹¨ìˆœí•œ ê²½ìš°, ëª¨ë“  í”„ë¡œì íŠ¸ í´ë”ì—ì„œ ê²€ìƒ‰
        const jobs = await this.getJobs();
        const matchingJob = jobs.find(job => job.name === jobName);
        if (matchingJob) {
          folderPath = matchingJob.folderPath || matchingJob.projectFolder;
          actualJobName = matchingJob.name;
        } else {
          logger.warn(`Job ${jobName} not found in any project folder`);
          return [];
        }
      }

      // ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ì— ë§ëŠ” URL ìƒì„± - projects ì ‘ë‘ì‚¬ í¬í•¨
      const folderPathParts = folderPath.split('/');
      const jobPath = folderPathParts.map(part => `/job/${encodeURIComponent(part)}`).join('');
      const url = `/job/projects${jobPath}/job/${encodeURIComponent(actualJobName)}/api/json?tree=builds[number,url,result,timestamp,duration,displayName,actions[parameters[name,value]],changeSet[items[commitId,msg,author[fullName]]]]&depth=2`;

      logger.debug(`Fetching builds for job ${folderPath}/${actualJobName} from URL: ${url}`);

      const response = await this.client.get(url);

      if (!response.data || !response.data.builds) {
        logger.warn(`No builds found for job ${folderPath}/${actualJobName}`);
        return [];
      }

      const builds = response.data.builds.slice(0, limit);
      logger.info(`Found ${builds.length} builds for job ${folderPath}/${actualJobName}`);

      return builds.map(build => ({
        id: build.number,
        projectName: `${folderPath}/${actualJobName}`,
        buildNumber: build.number,
        status: this.mapJenkinsStatus(build.result),
        timestamp: new Date(build.timestamp),
        duration: build.duration ? Math.round(build.duration / 1000) : null,
        displayName: build.displayName,
        url: build.url,
        parameters: this.extractParameters(build.actions),
        changes: this.extractChanges(build.changeSet),
      }));
    } catch (error) {
      logger.error(`Failed to fetch builds for job ${jobName}:`, error.message);
      logger.error(`Error status: ${error.response?.status}`);

      // 404ë‚˜ íŠ¹ì • ì—ëŸ¬ì˜ ê²½ìš° ë¹ˆ ë°°ì—´ ë°˜í™˜í•˜ì—¬ ë‹¤ë¥¸ ì‘ì—… ì¡°íšŒì— ì˜í–¥ì£¼ì§€ ì•ŠìŒ
      if (error.response?.status === 404) {
        logger.warn(`Job ${jobName} not found, returning empty builds`);
        return [];
      }

      throw new Error(`Jenkins ì‘ì—… ${jobName}ì˜ ë¹Œë“œ ì´ë ¥ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
    }
  }

  async getBuildDetails(jobName, buildNumber) {
    return withJenkinsRetry(async () => {
      // projects í´ë” í•˜ìœ„ì˜ ì‘ì—… ë¹Œë“œ ìƒì„¸ ì¡°íšŒ
      const response = await this.client.get(`/job/projects/job/${encodeURIComponent(jobName)}/${buildNumber}/api/json?depth=2`);
      const build = response.data;

      return {
        id: build.number,
        projectName: jobName,
        buildNumber: build.number,
        status: this.mapJenkinsStatus(build.result),
        timestamp: new Date(build.timestamp),
        duration: build.duration ? Math.round(build.duration / 1000) : null,
        displayName: build.displayName,
        url: build.url,
        description: build.description,
        parameters: this.extractParameters(build.actions),
        changes: this.extractChanges(build.changeSet),
        artifacts: build.artifacts || [],
        building: build.building || false,
        result: build.result,
        queueId: build.queueId,
      };
    }, {}, `Jenkins getBuildDetails: ${jobName}#${buildNumber}`);
  }

  async getBuildLog(jobName, buildNumber) {
    return withJenkinsRetry(async () => {
      // BE/FE jobì¸ ê²½ìš° MR jobì— ì˜í•´ íŠ¸ë¦¬ê±°ëœ downstream ë¹Œë“œ ë²ˆí˜¸ë¥¼ ì°¾ê¸°
      let actualBuildNumber = buildNumber;
      if (jobName.includes('/be') || jobName.includes('/fe')) {
        actualBuildNumber = await this.findDownstreamBuildNumber(jobName, buildNumber);
        if (!actualBuildNumber) {
          logger.warn(`No downstream build found for ${jobName} triggered by MR build ${buildNumber}`);
          return [{
            message: `${jobName.includes('/be') ? 'BE' : 'FE'} ë¹Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. MR ë¹Œë“œ #${buildNumber}ì— ì˜í•´ íŠ¸ë¦¬ê±°ëœ downstream ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.`,
            level: 'info',
            timestamp: new Date().toISOString(),
          }];
        }
        logger.info(`Found downstream build ${actualBuildNumber} for ${jobName} (triggered by MR build ${buildNumber})`);
      }

      // ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ë¥¼ Jenkins API ê²½ë¡œë¡œ ë³€í™˜
      // ì˜ˆ: "3.0.0/mr3.0.0_release" -> "/job/projects/job/3.0.0/job/mr3.0.0_release"
      const jobPath = jobName.split('/').map(part => `/job/${encodeURIComponent(part)}`).join('');
      const fullPath = `/job/projects${jobPath}/${actualBuildNumber}/consoleText`;

      logger.debug(`Fetching Jenkins log from: ${fullPath}`);

      const response = await this.client.get(fullPath);

      const logContent = response.data;
      logger.info(`Retrieved ${logContent.length} characters of log data for ${jobName}#${actualBuildNumber}`);

      // ë¡œê·¸ë¥¼ ì¤„ë³„ë¡œ ë¶„ë¦¬í•˜ê³  íŒŒì‹±
      const logs = this.parseJenkinsConsoleLog(logContent, jobName, actualBuildNumber);

      logger.info(`Parsed ${logs.length} log entries for ${jobName}#${actualBuildNumber}`);

      return logs;
    }, {}, `Jenkins getBuildLog: ${jobName}#${buildNumber}`);
  }

  /**
   * MR jobì— ì˜í•´ íŠ¸ë¦¬ê±°ëœ downstream ë¹Œë“œ ë²ˆí˜¸ ì°¾ê¸°
   */
  async findDownstreamBuildNumber(downstreamJobName, upstreamBuildNumber) {

    try {
      // upstream job name êµ¬ì„± (BE/FE -> MR)
      const upstreamJobName = downstreamJobName.replace(/(be|fe)(\d+\.\d+\.\d+)_release/, 'mr$2_release');
      logger.debug(`Searching for downstream builds: ${downstreamJobName} triggered by ${upstreamJobName}#${upstreamBuildNumber}`);

      const jobPath = downstreamJobName.split('/').map(part => `/job/${encodeURIComponent(part)}`).join('');
      const buildsUrl = `/job/projects${jobPath}/api/json?tree=builds[number,actions[causes[upstreamBuild,upstreamProject]]]`;

      const response = await this.client.get(buildsUrl);
      const builds = response.data.builds || [];

      for (const build of builds) {
        const causes = build.actions?.find(action => action.causes)?.causes || [];
        for (const cause of causes) {
          // íƒ€ì… ì•ˆì „ ë¹„êµ: ìˆ«ìë¡œ ë³€í™˜í•´ì„œ ë¹„êµ
          const causeUpstreamBuild = parseInt(cause.upstreamBuild);
          const targetUpstreamBuild = parseInt(upstreamBuildNumber);

          if (causeUpstreamBuild === targetUpstreamBuild &&
              cause.upstreamProject &&
              cause.upstreamProject === `projects/${upstreamJobName}`) {
            logger.info(`Found downstream build ${build.number} for ${downstreamJobName} triggered by ${upstreamJobName}#${upstreamBuildNumber}`);
            return build.number;
          }
        }
      }

      return null;
    } catch (error) {
      logger.error(`Failed to find downstream build for ${downstreamJobName}:`, error.message);
      return null;
    }
  }

  /**
   * ë¹Œë“œ ë¡œê·¸ì—ì„œ ì••ì¶• íŒŒì¼ ì •ë³´ ì¶”ì¶œ
   */
  async extractArtifactsFromBuildLog(jobName, buildNumber) {
    try {
      // ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ë¥¼ Jenkins API ê²½ë¡œë¡œ ë³€í™˜
      const jobPath = jobName.split('/').map(part => `/job/${encodeURIComponent(part)}`).join('');
      const fullPath = `/job/projects${jobPath}/${buildNumber}/consoleText`;

      logger.debug(`Extracting artifacts from Jenkins log: ${fullPath}`);

      const response = await this.client.get(fullPath);
      const logContent = response.data;

      const artifacts = [];
      const lines = logContent.split('\n');

      // ì••ì¶• íŒŒì¼ ê´€ë ¨ íŒ¨í„´ë“¤
      const artifactPatterns = [
        // tar.gz íŒŒì¼ ìƒì„±/ë³µì‚¬ íŒ¨í„´
        /(?:created?|generated?|copied?|built?|archived?).*?([a-zA-Z0-9_\-\.]+\.tar\.gz)/gi,
        // zip íŒŒì¼ íŒ¨í„´
        /(?:created?|generated?|copied?|built?|archived?).*?([a-zA-Z0-9_\-\.]+\.zip)/gi,
        // 7z íŒŒì¼ íŒ¨í„´
        /(?:created?|generated?|copied?|built?|archived?).*?([a-zA-Z0-9_\-\.]+\.7z)/gi,
        // ì••ì¶• íŒŒì¼ ê²½ë¡œ íŒ¨í„´
        /([\/\w\-\.]+\/[a-zA-Z0-9_\-\.]+\.(tar\.gz|zip|7z))/gi,
        // Archiving artifacts íŒ¨í„´ (Jenkins ê¸°ë³¸ ì•„ì¹´ì´ë¹™)
        /Archiving artifacts.*?([a-zA-Z0-9_\-\.]+\.(tar\.gz|zip|7z))/gi,
        // tar ëª…ë ¹ì–´ íŒ¨í„´
        /tar.*?-[czf]+.*?([a-zA-Z0-9_\-\.]+\.tar\.gz)/gi,
        // íŒŒì¼ëª…ë§Œ ìˆëŠ” íŒ¨í„´ (mr1.2.0_release_1.2.0.tar.gz í˜•ì‹)
        /([a-zA-Z0-9_\-\.]+_release_[0-9\.]+\.(tar\.gz|zip|7z))/gi,
      ];

      // ê° ë¼ì¸ì—ì„œ ì•„í‹°íŒ©íŠ¸ íŒŒì¼ëª… ì¶”ì¶œ
      for (let i = 0; i < lines.length; i++) {
        const line = lines[i];

        for (const pattern of artifactPatterns) {
          let match;
          pattern.lastIndex = 0; // ì •ê·œì‹ ìƒíƒœ ì´ˆê¸°í™”

          while ((match = pattern.exec(line)) !== null) {
            const filename = match[1];

            // ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì‚¬
            if (filename && !artifacts.find(a => a.filename === filename)) {
              const artifact = {
                filename: filename,
                buildNumber: buildNumber,
                jobName: jobName,
                foundInLine: i + 1,
                context: line.trim(),
                extractedAt: new Date().toISOString(),
              };

              // íŒŒì¼ í¬ê¸° ì¶”ì¶œ ì‹œë„ (ê°™ì€ ë¼ì¸ì´ë‚˜ ì£¼ë³€ ë¼ì¸ì—ì„œ)
              const sizeMatch = line.match(/(\d+(?:\.\d+)?)\s*(KB|MB|GB|bytes?)/i);
              if (sizeMatch) {
                artifact.size = sizeMatch[0];
              }

              // NAS ê²½ë¡œ ì¶”ì¶œ ì‹œë„
              const pathMatch = line.match(/([\/\w\-\.]+)\/[a-zA-Z0-9_\-\.]+\.(tar\.gz|zip|7z)/i);
              if (pathMatch) {
                artifact.nasPath = pathMatch[1];
              }

              artifacts.push(artifact);
              logger.info(`Extracted artifact from build log: ${filename} (line ${i + 1})`);
            }
          }
        }
      }

      // ë¹Œë“œ ë²ˆí˜¸ì™€ í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ì˜ˆìƒ íŒŒì¼ëª… ìƒì„± (ì¶”ê°€ ê²€ì¦ìš©)
      const expectedArtifacts = this.generateExpectedArtifacts(jobName, buildNumber);
      for (const expected of expectedArtifacts) {
        if (!artifacts.find(a => a.filename === expected.filename)) {
          artifacts.push({
            ...expected,
            foundInLine: null,
            context: 'Generated from job pattern',
            extractedAt: new Date().toISOString(),
          });
        }
      }

      logger.info(`Extracted ${artifacts.length} artifacts from build log for ${jobName}#${buildNumber}`);
      return artifacts;

    } catch (error) {
      logger.error(`Failed to extract artifacts from build log for ${jobName}#${buildNumber}:`, error.message);
      throw new Error(`ë¹Œë“œ ë¡œê·¸ì—ì„œ ì•„í‹°íŒ©íŠ¸ ì •ë³´ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: ${error.message}`);
    }
  }

  /**
   * ì‘ì—…ëª…ê³¼ ë¹Œë“œ ë²ˆí˜¸ë¡œ ì˜ˆìƒ ì•„í‹°íŒ©íŠ¸ íŒŒì¼ëª… ìƒì„±
   */
  generateExpectedArtifacts(jobName, buildNumber) {
    const artifacts = [];

    // jobNameì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
    const versionMatch = jobName.match(/(\d+\.\d+\.\d+)/);
    if (versionMatch) {
      const version = versionMatch[1];
      const jobType = jobName.toLowerCase().includes('mr') ? 'mr' : 'fs';

      // ì¼ë°˜ì ì¸ ì•„í‹°íŒ©íŠ¸ íŒŒì¼ëª… íŒ¨í„´ë“¤
      const patterns = [
        `${jobType}${version}_release_${version}.tar.gz`,
        `${jobName}_${buildNumber}.tar.gz`,
        `${jobName}.tar.gz`,
        `release_${version}.tar.gz`,
      ];

      for (const filename of patterns) {
        artifacts.push({
          filename: filename,
          buildNumber: buildNumber,
          jobName: jobName,
          type: 'expected',
        });
      }
    }

    return artifacts;
  }

  /**
   * ìƒˆë¡œìš´ fallback chainì„ ì‚¬ìš©í•œ ë°°í¬ ì •ë³´ ì¶”ì¶œ
   * 1. DBì—ì„œ ìºì‹œëœ ê²½ë¡œ ì¡°íšŒ
   * 2. Jenkins APIì—ì„œ ë¹Œë“œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
   * 3. ë‚ ì§œ ê¸°ë°˜ ê²½ë¡œ í›„ë³´ ìƒì„±
   * 4. NAS ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ê²€ì¦
   * 5. ì„±ê³µ ì‹œ DBì— ì €ì¥
   */
  async extractDeploymentInfo(jobName, buildNumber) {
    const startTime = Date.now();
    const requestId = `${jobName}#${buildNumber}-${Date.now()}`;
    const metricsService = getMetricsService();

    try {
      logger.info(`[${requestId}] Starting deployment info extraction for ${jobName}#${buildNumber}`, {
        jobName,
        buildNumber,
        requestId,
        timestamp: new Date().toISOString(),
      });

      // Step 1: DB ìºì‹œ ì¡°íšŒ
      logger.debug(`[${requestId}] Step 1: Checking database cache`, { step: 'cache_lookup' });
      const cacheStartTime = Date.now();
      const cachedPath = await this.checkDatabaseCache(jobName, buildNumber);
      const cacheTime = Date.now() - cacheStartTime;

      if (cachedPath) {
        // Cache hit
        logger.info(`[${requestId}] Cache hit - found cached deployment path`, {
          step: 'cache_hit',
          nasPath: cachedPath.nasPath,
          downloadFile: cachedPath.downloadFile,
          cacheResponseTime: `${cacheTime}ms`,
          totalTime: `${Date.now() - startTime}ms`,
        });

        metricsService.recordRequest(true);

        // Record success for alerting
        const alertingService = getAlertingService();
        alertingService.recordPathDetectionSuccess();

        return cachedPath;
      }

      // Cache miss
      logger.debug(`[${requestId}] Cache miss - proceeding to dynamic path detection`, {
        step: 'cache_miss',
        cacheResponseTime: `${cacheTime}ms`,
      });

      // Step 2: Jenkins APIì—ì„œ ë¹Œë“œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
      logger.debug(`[${requestId}] Step 2: Extracting build timestamp from Jenkins API`, { step: 'jenkins_api' });
      const apiStartTime = Date.now();
      const buildTimestamp = await this.getBuildTimestamp(jobName, buildNumber);
      const apiTime = Date.now() - apiStartTime;

      if (!buildTimestamp) {
        metricsService.recordError('api');
        logger.warn(`[${requestId}] Jenkins API failed - falling back to build log extraction`, {
          step: 'jenkins_api_failed',
          apiResponseTime: `${apiTime}ms`,
          fallbackReason: 'no_build_timestamp',
        });

        const legacyResult = await this.extractDeploymentInfoFromBuildLog(jobName, buildNumber);
        metricsService.recordRequest(!!legacyResult);

        // Record alerting data for Jenkins API failure fallback
        const alertingService = getAlertingService();
        if (legacyResult) {
          alertingService.recordPathDetectionSuccess();
        } else {
          await alertingService.recordPathDetectionFailure({
            projectName: jobName,
            version: 'unknown',
            buildNumber,
            reason: 'jenkins_api_and_legacy_both_failed',
            responseTime: Date.now() - startTime,
          });
        }

        return legacyResult;
      }

      // API call success

      logger.debug(`[${requestId}] Build timestamp extracted successfully`, {
        step: 'jenkins_api_success',
        buildTimestamp: buildTimestamp.toISOString(),
        apiResponseTime: `${apiTime}ms`,
      });

      // Step 3: ê²½ë¡œ í›„ë³´ ìƒì„±
      logger.debug(`[${requestId}] Step 3: Generating NAS path candidates`, { step: 'path_generation' });
      const pathGenStartTime = Date.now();
      const pathCandidates = await this.generateNASPathCandidates(jobName, buildNumber, buildTimestamp);
      const pathGenTime = Date.now() - pathGenStartTime;

      // Path generation complete

      if (pathCandidates.length === 0) {
        metricsService.recordError('path_generation');
        logger.warn(`[${requestId}] No path candidates generated - falling back to build log extraction`, {
          step: 'path_generation_failed',
          candidateCount: 0,
          fallbackReason: 'no_path_candidates',
        });

        const legacyResult = await this.extractDeploymentInfoFromBuildLog(jobName, buildNumber);
        metricsService.recordRequest(!!legacyResult);

        // Record alerting data for path generation failure fallback
        const alertingService = getAlertingService();
        if (legacyResult) {
          alertingService.recordPathDetectionSuccess();
        } else {
          await alertingService.recordPathDetectionFailure({
            projectName: jobName,
            version: 'unknown',
            buildNumber,
            reason: 'no_path_candidates_and_legacy_failed',
            responseTime: Date.now() - startTime,
          });
        }

        return legacyResult;
      }

      logger.debug(`[${requestId}] Generated ${pathCandidates.length} path candidates`, {
        step: 'path_generation_success',
        candidateCount: pathCandidates.length,
        candidates: pathCandidates.map(c => c.nasPath).slice(0, 3), // Log first 3 for debugging
        pathGenTime: `${pathGenTime}ms`,
      });

      // Step 4: NAS ë””ë ‰í† ë¦¬ ìŠ¤ìº” ë° ê²€ì¦
      logger.debug(`[${requestId}] Step 4: Verifying NAS paths`, { step: 'nas_verification' });
      const nasStartTime = Date.now();
      const verifiedPath = await this.verifyNASPaths(pathCandidates);
      const nasTime = Date.now() - nasStartTime;

      if (!verifiedPath) {
        metricsService.recordError('nas_verification');
        logger.warn(`[${requestId}] NAS verification failed - falling back to build log extraction`, {
          step: 'nas_verification_failed',
          nasResponseTime: `${nasTime}ms`,
          candidatesChecked: pathCandidates.length,
          fallbackReason: 'no_verified_path',
        });

        const legacyResult = await this.extractDeploymentInfoFromBuildLog(jobName, buildNumber);
        metricsService.recordRequest(!!legacyResult);

        // Record alerting data for NAS verification failure fallback
        const alertingService = getAlertingService();
        if (legacyResult) {
          alertingService.recordPathDetectionSuccess();
        } else {
          await alertingService.recordPathDetectionFailure({
            projectName: jobName,
            version: 'unknown',
            buildNumber,
            reason: 'nas_verification_and_legacy_both_failed',
            responseTime: Date.now() - startTime,
          });
        }

        return legacyResult;
      }

      // NAS verification success
      logger.debug(`[${requestId}] NAS path verified successfully`, {
        step: 'nas_verification_success',
        verifiedPath: verifiedPath.nasPath,
        fileCount: verifiedPath.allFiles?.length || 0,
        mainFile: verifiedPath.downloadFile,
        nasResponseTime: `${nasTime}ms`,
      });

      // Step 5: ì„±ê³µ ì‹œ DBì— ì €ì¥
      logger.debug(`[${requestId}] Step 5: Saving to database cache`, { step: 'cache_save' });
      const saveStartTime = Date.now();
      try {
        await this.saveDeploymentPathToCache(jobName, buildNumber, buildTimestamp, verifiedPath);
        const saveTime = Date.now() - saveStartTime;
        // Database save success

        const totalTime = Date.now() - startTime;
        logger.info(`[${requestId}] Successfully extracted deployment info`, {
          step: 'complete_success',
          nasPath: verifiedPath.nasPath,
          downloadFile: verifiedPath.downloadFile,
          fileCount: verifiedPath.allFiles?.length || 0,
          performance: {
            totalTime: `${totalTime}ms`,
            cacheTime: `${cacheTime}ms`,
            apiTime: `${apiTime}ms`,
            pathGenTime: `${pathGenTime}ms`,
            nasTime: `${nasTime}ms`,
            saveTime: `${saveTime}ms`,
          },
          pathCandidatesGenerated: pathCandidates.length,
        });

        metricsService.recordRequest(true);

        // Record success for alerting
        const alertingService = getAlertingService();
        alertingService.recordPathDetectionSuccess();

        return verifiedPath;
      } catch (saveError) {
        const saveTime = Date.now() - saveStartTime;
        metricsService.recordError('db');
        logger.warn(`[${requestId}] Failed to save to cache, but continuing with result`, {
          step: 'cache_save_failed',
          error: saveError.message,
          saveTime: `${saveTime}ms`,
        });

        metricsService.recordRequest(true);
        return verifiedPath;
      }

    } catch (error) {
      const totalTime = Date.now() - startTime;
      metricsService.recordError('unknown');

      logger.error(`[${requestId}] Error in deployment info extraction - falling back to legacy method`, {
        step: 'error_fallback',
        error: error.message,
        errorStack: error.stack,
        totalTime: `${totalTime}ms`,
        fallbackReason: 'exception_caught',
      });

      // ìƒˆë¡œìš´ ë°©ì‹ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±
      const legacyResult = await this.extractDeploymentInfoFromBuildLog(jobName, buildNumber);
      metricsService.recordRequest(!!legacyResult);

      // Record alerting data for exception fallback
      const alertingService = getAlertingService();
      if (legacyResult) {
        alertingService.recordPathDetectionSuccess();
      } else {
        await alertingService.recordPathDetectionFailure({
          projectName: jobName,
          version: 'unknown',
          buildNumber,
          reason: 'exception_and_legacy_both_failed',
          responseTime: totalTime,
        });
      }

      return legacyResult;
    }
  }

  /**
   * DBì—ì„œ ìºì‹œëœ ë°°í¬ ê²½ë¡œ ì¡°íšŒ
   */
  async checkDatabaseCache(jobName, buildNumber) {
    try {
      const deploymentPathService = getDeploymentPathService();

      // jobNameì—ì„œ ë²„ì „ ì¶”ì¶œ
      const versionMatch = jobName.match(/(\d+\.\d+\.\d+)/);
      if (!versionMatch) {
        logger.warn('Cache lookup failed - could not extract version from job name', {
          jobName,
          buildNumber,
          reason: 'invalid_job_name_format',
        });
        return null;
      }

      const version = versionMatch[1];
      logger.debug('Searching database cache', {
        jobName,
        version,
        buildNumber,
        operation: 'cache_lookup',
      });

      const cachedPath = await deploymentPathService.findByProjectVersionBuild(
        jobName,
        version,
        buildNumber,
      );

      if (cachedPath) {
        logger.debug('Database cache hit found', {
          jobName,
          version,
          buildNumber,
          cachedPath: cachedPath.nasPath,
          createdAt: cachedPath.createdAt,
          fileCount: cachedPath.allFiles?.length || 0,
        });

        return {
          nasPath: cachedPath.nasPath,
          downloadFile: cachedPath.downloadFile,
          allFiles: cachedPath.allFiles || [],
          deploymentPath: cachedPath.nasPath,
        };
      }

      logger.debug('Database cache miss', {
        jobName,
        version,
        buildNumber,
        operation: 'cache_miss',
      });

      return null;
    } catch (error) {
      logger.error('Database cache lookup failed with error', {
        jobName,
        buildNumber,
        error: error.message,
        errorStack: error.stack,
        operation: 'cache_error',
      });
      return null;
    }
  }

  /**
   * Jenkins APIì—ì„œ ë¹Œë“œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
   */
  async getBuildTimestamp(jobName, buildNumber) {
    return withJenkinsRetry(async () => {
      try {
        // ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ë¥¼ Jenkins API ê²½ë¡œë¡œ ë³€í™˜
        const jobPath = jobName.split('/').map(part => `/job/${encodeURIComponent(part)}`).join('');
        const apiUrl = `/job/projects${jobPath}/${buildNumber}/api/json?tree=timestamp`;

        logger.debug(`Fetching build timestamp from: ${apiUrl}`);

        const response = await this.client.get(apiUrl);

        if (response.data && response.data.timestamp) {
          const buildDate = new Date(response.data.timestamp);
          logger.debug(`Build timestamp for ${jobName}#${buildNumber}: ${buildDate.toISOString()}`);
          return buildDate;
        }

        logger.warn(`No timestamp found in Jenkins API response for ${jobName}#${buildNumber}`);
        return null;
      } catch (error) {
        logger.error(`Failed to get build timestamp for ${jobName}#${buildNumber}: ${error.message}`);
        return null;
      }
    }, {}, `getBuildTimestamp: ${jobName}#${buildNumber}`);
  }

  /**
   * ë¹Œë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ NAS ê²½ë¡œ í›„ë³´ë“¤ ìƒì„±
   */
  async generateNASPathCandidates(jobName, buildNumber, buildTimestamp) {
    try {
      // jobNameì—ì„œ ë²„ì „ ì¶”ì¶œ
      const versionMatch = jobName.match(/(\d+\.\d+\.\d+)/);
      if (!versionMatch) {
        logger.warn(`Could not extract version from job name: ${jobName}`);
        return [];
      }

      const version = versionMatch[1];

      // ë¹Œë“œ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ Â±1ì¼ ë²”ìœ„ì˜ ë‚ ì§œ í›„ë³´ë“¤ ìƒì„±
      const dateCandidates = generatePathCandidates(buildTimestamp, 1);

      const pathCandidates = [];

      // ê° ë‚ ì§œ í›„ë³´ì— ëŒ€í•´ ê²½ë¡œ ìƒì„±
      for (const dateStr of dateCandidates) {
        try {
          const nasPath = constructNASPath(version, dateStr, buildNumber);
          pathCandidates.push({
            nasPath,
            dateStr,
            version,
            buildNumber,
          });
        } catch (error) {
          logger.warn(`Failed to construct NAS path for ${version}, ${dateStr}, ${buildNumber}: ${error.message}`);
        }
      }

      logger.debug(`Generated ${pathCandidates.length} path candidates for ${jobName}#${buildNumber}`);
      return pathCandidates;
    } catch (error) {
      logger.error(`Error generating path candidates for ${jobName}#${buildNumber}: ${error.message}`);
      return [];
    }
  }

  /**
   * NAS ê²½ë¡œë“¤ì„ ê²€ì¦í•˜ê³  ì²« ë²ˆì§¸ë¡œ ì¡´ì¬í•˜ëŠ” ê²½ë¡œì˜ íŒŒì¼ ëª©ë¡ ë°˜í™˜
   */
  async verifyNASPaths(pathCandidates) {
    try {
      const nasService = getNASService();

      let pathsChecked = 0;
      let pathsSkipped = 0;
      const verificationResults = [];

      for (const candidate of pathCandidates) {
        try {
          pathsChecked++;
          const candidateStartTime = Date.now();

          logger.debug(`Verifying NAS path candidate ${pathsChecked}/${pathCandidates.length}`, {
            candidatePath: candidate.nasPath,
            candidateDate: candidate.dateStr,
            candidateBuild: candidate.buildNumber,
            operation: 'nas_path_check',
          });

          // ë””ë ‰í† ë¦¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
          const existsStartTime = Date.now();
          const exists = await withNASRetry(async () => {
            return await nasService.directoryExists(candidate.nasPath);
          }, {}, `NAS directoryExists: ${candidate.nasPath}`);
          const existsTime = Date.now() - existsStartTime;

          if (!exists) {
            pathsSkipped++;
            logger.debug('NAS path does not exist - skipping', {
              candidatePath: candidate.nasPath,
              checkTime: `${existsTime}ms`,
              result: 'not_found',
              operation: 'nas_directory_check',
            });
            verificationResults.push({
              path: candidate.nasPath,
              result: 'not_found',
              time: existsTime,
            });
            continue;
          }

          logger.debug('NAS directory exists - checking files', {
            candidatePath: candidate.nasPath,
            checkTime: `${existsTime}ms`,
            result: 'found',
            operation: 'nas_directory_found',
          });

          // ë””ë ‰í† ë¦¬ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
          const filesStartTime = Date.now();
          const files = await withNASRetry(async () => {
            return await nasService.getDirectoryFiles(candidate.nasPath);
          }, {}, `NAS getDirectoryFiles: ${candidate.nasPath}`);
          const filesTime = Date.now() - filesStartTime;

          if (!files || files.length === 0) {
            pathsSkipped++;
            logger.debug('NAS directory is empty - skipping', {
              candidatePath: candidate.nasPath,
              filesCheckTime: `${filesTime}ms`,
              result: 'empty_directory',
              operation: 'nas_files_check',
            });
            verificationResults.push({
              path: candidate.nasPath,
              result: 'empty',
              time: existsTime + filesTime,
            });
            continue;
          }

          // íŒŒì¼ ë¶„ë¥˜ ë° ë©”ì¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ê²°ì •
          const categorized = categorizeFiles(files);
          const mainDownloadFile = determineMainDownloadFile(files);

          const candidateTime = Date.now() - candidateStartTime;

          logger.info('NAS path verification successful', {
            verifiedPath: candidate.nasPath,
            fileCount: files.length,
            mainDownloadFile,
            categorizedFiles: {
              versionFiles: categorized.versionFiles?.length || 0,
              mrFiles: categorized.mrFiles?.length || 0,
              backendFiles: categorized.backendFiles?.length || 0,
              frontendFiles: categorized.frontendFiles?.length || 0,
              otherFiles: categorized.otherFiles?.length || 0,
            },
            verificationTime: `${candidateTime}ms`,
            pathsChecked,
            pathsSkipped,
            operation: 'nas_verification_success',
          });

          return {
            nasPath: candidate.nasPath,
            deploymentPath: candidate.nasPath,
            downloadFile: mainDownloadFile,
            allFiles: files,
            categorized,
            directoryVerified: true,
            downloadFileVerified: !!mainDownloadFile,
          };

        } catch (error) {
          pathsSkipped++;
          const candidateTime = Date.now() - candidateStartTime;

          logger.warn('NAS path verification failed with error', {
            candidatePath: candidate.nasPath,
            error: error.message,
            verificationTime: `${candidateTime}ms`,
            pathsChecked,
            pathsSkipped,
            operation: 'nas_verification_error',
          });

          verificationResults.push({
            path: candidate.nasPath,
            result: 'error',
            error: error.message,
            time: candidateTime,
          });
          continue;
        }
      }

      logger.warn('No valid NAS paths found after verification', {
        totalCandidates: pathCandidates.length,
        pathsChecked,
        pathsSkipped,
        verificationResults: verificationResults.slice(0, 5), // Log first 5 for debugging
        operation: 'nas_verification_complete_failure',
      });

      return null;
    } catch (error) {
      logger.error('Critical error during NAS path verification', {
        error: error.message,
        errorStack: error.stack,
        candidateCount: pathCandidates.length,
        operation: 'nas_verification_critical_error',
      });
      return null;
    }
  }

  /**
   * ê²€ì¦ëœ ë°°í¬ ê²½ë¡œë¥¼ DB ìºì‹œì— ì €ì¥
   */
  async saveDeploymentPathToCache(jobName, buildNumber, buildTimestamp, verifiedPath) {
    try {
      const deploymentPathService = getDeploymentPathService();

      // jobNameì—ì„œ ë²„ì „ ì¶”ì¶œ
      const versionMatch = jobName.match(/(\d+\.\d+\.\d+)/);
      if (!versionMatch) {
        logger.warn(`Could not extract version from job name for caching: ${jobName}`);
        return;
      }

      const version = versionMatch[1];

      const pathData = {
        projectName: jobName,
        version: version,
        buildNumber: buildNumber,
        buildDate: buildTimestamp,
        nasPath: verifiedPath.nasPath,
        downloadFile: verifiedPath.downloadFile,
        allFiles: verifiedPath.allFiles || [],
      };

      await deploymentPathService.saveDeploymentPath(pathData);
      logger.info(`Saved deployment path to cache: ${jobName}#${buildNumber}`);

    } catch (error) {
      logger.error(`Failed to save deployment path to cache for ${jobName}#${buildNumber}: ${error.message}`);
      // ìºì‹œ ì €ì¥ ì‹¤íŒ¨ëŠ” ì¹˜ëª…ì ì´ì§€ ì•Šìœ¼ë¯€ë¡œ ì—ëŸ¬ë¥¼ ë˜ì§€ì§€ ì•ŠìŒ
    }
  }

  /**
   * ë™ì  í´ë°± ê²½ë¡œ ìƒì„± (í•˜ë“œì½”ë”©ëœ ë‚ ì§œ ë§¤í•‘ ëŒ€ì‹  ì‚¬ìš©)
   */
  async generateDynamicFallbackPath(jobName, buildNumber) {
    try {
      // ë¨¼ì € Jenkins APIì—ì„œ ë¹Œë“œ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê°€ì ¸ì™€ ë³´ê¸°
      const buildTimestamp = await this.getBuildTimestamp(jobName, buildNumber);

      if (buildTimestamp) {
        // íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆìœ¼ë©´ path detection ìœ í‹¸ë¦¬í‹° ì‚¬ìš©
        const pathCandidates = await this.generateNASPathCandidates(jobName, buildNumber, buildTimestamp);
        if (pathCandidates.length > 0) {
          return pathCandidates[0]; // ì²« ë²ˆì§¸ í›„ë³´ ì‚¬ìš©
        }
      }

      // íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° í˜„ì¬ ë‚ ì§œ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
      const versionMatch = jobName.match(/(\d+\.\d+\.\d+)/);
      if (versionMatch) {
        const version = versionMatch[1];
        const today = new Date();

        try {
          const dateStr = formatDateForNAS(today);
          const nasPath = constructNASPath(version, dateStr, buildNumber);

          logger.info(`Generated fallback path using current date: ${nasPath}`);
          return {
            nasPath,
            dateStr,
            version,
            buildNumber,
          };
        } catch (error) {
          logger.warn(`Failed to generate fallback path: ${error.message}`);
        }
      }

      return null;
    } catch (error) {
      logger.error(`Error generating dynamic fallback path for ${jobName}#${buildNumber}: ${error.message}`);
      return null;
    }
  }

  /**
   * NAS ê²½ë¡œì—ì„œ íŒŒì¼ ëª©ë¡ì„ ì¶”ë¡ í•˜ì—¬ ë™ì ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
   */
  async inferFilesFromPath(nasPath, jobName, buildNumber) {
    try {
      // NAS ê²½ë¡œì—ì„œ ë‚ ì§œ ë° ë²„ì „ ì •ë³´ ì¶”ì¶œ
      const pathMatch = nasPath.match(/\\mr(\d+\.\d+\.\d+)\\(\d{6})\\(\d+)$/);
      if (!pathMatch) {
        logger.warn(`Could not parse NAS path format: ${nasPath}`);
        return null;
      }

      const [, version, dateStr] = pathMatch;

      // NAS ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤ì œ íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹œë„
      try {
        const nasService = getNASService();
        const files = await withNASRetry(async () => {
          return await nasService.getDirectoryFiles(nasPath);
        }, {}, `NAS getDirectoryFiles: ${nasPath}`);

        if (files && files.length > 0) {
          const mainDownloadFile = determineMainDownloadFile(files);
          logger.info(`Found ${files.length} files in ${nasPath}, main file: ${mainDownloadFile}`);

          return {
            downloadFile: mainDownloadFile,
            allFiles: files,
            directoryVerified: true,
            downloadFileVerified: !!mainDownloadFile,
          };
        }
      } catch (nasError) {
        logger.debug(`Could not access NAS directory ${nasPath}: ${nasError.message}`);
      }

      // NAS ì ‘ê·¼ ì‹¤íŒ¨ ì‹œ íŒ¨í„´ ê¸°ë°˜ íŒŒì¼ëª… ìƒì„±
      const expectedFiles = this.generateExpectedFilenamesByPattern(version, dateStr, buildNumber);
      if (expectedFiles.length > 0) {
        const mainFile = determineMainDownloadFile(expectedFiles);
        logger.info(`Generated expected files for ${nasPath}, main file: ${mainFile}`);

        return {
          downloadFile: mainFile,
          allFiles: expectedFiles,
          directoryVerified: false, // íŒ¨í„´ ê¸°ë°˜ì´ë¯€ë¡œ ì‹¤ì œ ê²€ì¦ë˜ì§€ ì•ŠìŒ
          downloadFileVerified: false, // íŒ¨í„´ ê¸°ë°˜ì´ë¯€ë¡œ ì‹¤ì œ ê²€ì¦ë˜ì§€ ì•ŠìŒ
        };
      }

      return null;
    } catch (error) {
      logger.error(`Error inferring files from path ${nasPath}: ${error.message}`);
      return null;
    }
  }

  /**
   * ë²„ì „, ë‚ ì§œ, ë¹Œë“œë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆìƒ íŒŒì¼ëª…ë“¤ ìƒì„±
   */
  generateExpectedFilenamesByPattern(version, dateStr, buildNumber) {
    const expectedFiles = [];

    try {
      // Vë²„ì „ íŒŒì¼ (ë©”ì¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼) - ì‹œê°„ì€ ì„ì˜ë¡œ ì„¤ì •
      const timeStr = '1000'; // ê¸°ë³¸ ì‹œê°„
      expectedFiles.push(`V${version}_${dateStr}_${timeStr}.tar.gz`);

      // MR íŒŒì¼ë“¤
      expectedFiles.push(`mr${version}_${dateStr}_${timeStr}_${buildNumber}.enc.tar.gz`);

      // BE/FE íŒŒì¼ë“¤ - ë¹Œë“œë²ˆí˜¸ëŠ” ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŒ¨í„´ìœ¼ë¡œ
      expectedFiles.push(`be${version}_${dateStr}_${timeStr}_${buildNumber + 1}.enc.tar.gz`);
      expectedFiles.push(`fe${version}_${dateStr}_${timeStr}_${buildNumber + 2}.enc.tar.gz`);

      logger.debug(`Generated ${expectedFiles.length} expected files for ${version}_${dateStr}_${buildNumber}`);

    } catch (error) {
      logger.warn(`Error generating expected filenames: ${error.message}`);
    }

    return expectedFiles;
  }

  /**
   * Jenkins ë¡œê·¸ì—ì„œ ì‹¤ì œ ë°°í¬ ê²½ë¡œì™€ ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì •ë³´ ì¶”ì¶œ (ê¸°ì¡´ ë°©ì‹ - í´ë°±ìš©)
   */
  async extractDeploymentInfoFromBuildLog(jobName, buildNumber) {
    try {
      // MR ë¹Œë“œ ë¡œê·¸ì—ì„œ ë°°í¬ ì •ë³´ ì¶”ì¶œ (fs ë¹Œë“œ ëŒ€ì‹  MR ë¹Œë“œì—ì„œ ì‹¤ì œ ë°°í¬ ì •ë³´ í™•ì¸)
      let targetJobName = jobName;

      // fs ì¡ì¸ ê²½ìš° ëŒ€ì‘ë˜ëŠ” mr ì¡ì„ ì°¾ì•„ì„œ ì‚¬ìš©
      if (jobName.includes('fs')) {
        targetJobName = jobName.replace(/fs(\d+\.\d+\.\d+)/, 'mr$1');
      }

      // ì¤‘ì²©ëœ í´ë” êµ¬ì¡°ë¥¼ Jenkins API ê²½ë¡œë¡œ ë³€í™˜
      const jobPath = targetJobName.split('/').map(part => `/job/${encodeURIComponent(part)}`).join('');
      const fullPath = `/job/projects${jobPath}/${buildNumber}/consoleText`;

      logger.debug(`Extracting deployment info from log: ${fullPath}`);

      try {
        const response = await withJenkinsRetry(async () => {
          return await this.client.get(fullPath);
        }, {}, `Jenkins getBuildLog: ${targetJobName}#${buildNumber}`);
        const logContent = response.data;
        const lines = logContent.split('\n');

      const deploymentInfo = {
        nasPath: null,
        downloadFile: null,
        allFiles: [],
        deploymentPath: null,
      };

      // NAS ë°°í¬ ê²½ë¡œ íŒ¨í„´ë“¤ (ì‹¤ì œ ë¡œê·¸ì—ì„œ ì¶”ì¶œ)
      const nasPathPatterns = [
        // \\nas.roboetech.com\release_version\release\product\mr3.0.0\250310\26
        /\\\\nas\.roboetech\.com\\release_version\\release\\product\\[^\\]+\\[^\\]+\\[^\\]+/gi,
        // /nas/release_version/release/product/mr3.0.0/250310/26
        /\/nas\/release_version\/release\/product\/[^\/]+\/[^\/]+\/[^\/]+/gi,
        // NAS ê²½ë¡œì˜ ë‹¤ì–‘í•œ í˜•ì‹
        /(?:copying|deploying|archiving).*?(?:to|at|in)\s+(\\\\nas\.roboetech\.com\\[^\\]+\\[^\\]+\\[^\\]+\\[^\\]+\\[^\\]+\\[^\\]+)/gi,
      ];

      // ë‹¤ìš´ë¡œë“œ íŒŒì¼ íŒ¨í„´ (V3.0.0_250310_0843.tar.gz í˜•ì‹)
      const downloadFilePatterns = [
        /V(\d+\.\d+\.\d+)_(\d+)_(\d+)\.tar\.gz/gi,
        // ë°±ì—… íŒ¨í„´ë“¤
        /([a-zA-Z0-9_\-\.]+)(?<!\.enc)\.tar\.gz/gi,
      ];

      // ëª¨ë“  íŒŒì¼ ëª©ë¡ íŒ¨í„´
      const allFilePatterns = [
        /(adam_\d+_\d+\.enc\.tar\.gz)/gi,                    // adam íŒŒì¼ íŒ¨í„´ ì¶”ê°€
        /(be\d+\.\d+\.\d+_\d+_\d+_\d+\.enc\.tar\.gz)/gi,
        /(fe\d+\.\d+\.\d+_\d+_\d+_\d+\.enc\.tar\.gz)/gi,
        /(mr\d+\.\d+\.\d+_\d+_\d+_\d+\.enc\.tar\.gz)/gi,
        /(V\d+\.\d+\.\d+_\d+_\d+\.tar\.gz)/gi,
      ];

      // ë¡œê·¸ì—ì„œ ì •ë³´ ì¶”ì¶œ
      for (let i = 0; i < lines.length; i++) {
        const line = lines[i];

        // NAS ê²½ë¡œ ì¶”ì¶œ
        for (const pattern of nasPathPatterns) {
          pattern.lastIndex = 0;
          const match = pattern.exec(line);
          if (match) {
            deploymentInfo.nasPath = match[0];
            deploymentInfo.deploymentPath = match[1] || match[0];
            logger.info(`Found NAS path: ${deploymentInfo.nasPath}`);
            break;
          }
        }

        // ë‹¤ìš´ë¡œë“œ íŒŒì¼ ì¶”ì¶œ
        for (const pattern of downloadFilePatterns) {
          pattern.lastIndex = 0;
          const match = pattern.exec(line);
          if (match && !match[0].includes('.enc.')) {
            deploymentInfo.downloadFile = match[0];
            logger.info(`Found download file: ${deploymentInfo.downloadFile}`);
            break;
          }
        }

        // ëª¨ë“  íŒŒì¼ ëª©ë¡ ì¶”ì¶œ
        for (const pattern of allFilePatterns) {
          pattern.lastIndex = 0;
          let match;
          while ((match = pattern.exec(line)) !== null) {
            if (!deploymentInfo.allFiles.includes(match[0])) {
              deploymentInfo.allFiles.push(match[0]);
            }
          }
        }
      }

      // ê²½ë¡œë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ë™ì  ê²½ë¡œ ìƒì„±
      if (!deploymentInfo.nasPath) {
        const fallbackPath = await this.generateDynamicFallbackPath(jobName, buildNumber);
        if (fallbackPath) {
          deploymentInfo.nasPath = fallbackPath.nasPath;
          deploymentInfo.deploymentPath = fallbackPath.nasPath;
          logger.info(`Generated dynamic fallback NAS path: ${deploymentInfo.nasPath}`);
        }
      }

      // ë©”ì¸ ë‹¤ìš´ë¡œë“œ íŒŒì¼ì´ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš° ë™ì  íŒŒì¼ëª… ì¶”ë¡ 
      if (!deploymentInfo.downloadFile && deploymentInfo.nasPath) {
        const dynamicFiles = await this.inferFilesFromPath(deploymentInfo.nasPath, jobName, buildNumber);
        if (dynamicFiles) {
          deploymentInfo.downloadFile = dynamicFiles.downloadFile;
          deploymentInfo.allFiles = dynamicFiles.allFiles;
          logger.info(`Inferred download file: ${deploymentInfo.downloadFile}`);
        }
      }

      // í•„ìš”í•œ ê²€ì¦ ì •ë³´ ì¶”ê°€
      deploymentInfo.directoryVerified = !!deploymentInfo.nasPath;
      deploymentInfo.downloadFileVerified = !!deploymentInfo.downloadFile;

      return deploymentInfo;

      } catch (fsError) {
        // fs ë¹Œë“œ ë¡œê·¸ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ëŠ” ê²½ìš° (404 ë“±)
        logger.warn(`Cannot access build log for ${jobName}#${buildNumber}: ${fsError.message}`);

        // ëª© ë°ì´í„° ì œê±° - ì‹¤ì œ ë¡œê·¸ì—ì„œë§Œ ì •ë³´ ì¶”ì¶œ
        return {
          nasPath: null,
          downloadFile: null,
          allFiles: [],
          deploymentPath: null,
          directoryVerified: false,
          downloadFileVerified: false,
        };
      }

    } catch (error) {
      logger.error(`Failed to extract deployment info for ${jobName}#${buildNumber}:`, error.message);

      // ì—ëŸ¬ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
      return {
        nasPath: null,
        downloadFile: null,
        allFiles: [],
        deploymentPath: null,
        directoryVerified: false,
        downloadFileVerified: false,
      };
    }
  }

  async triggerBuild(jobName, parameters = {}) {
    try {
      const hasParameters = Object.keys(parameters).length > 0;
      // projects í´ë” í•˜ìœ„ì˜ ì‘ì—… ë¹Œë“œ íŠ¸ë¦¬ê±°
      const endpoint = hasParameters
        ? `/job/projects/job/${encodeURIComponent(jobName)}/buildWithParameters`
        : `/job/projects/job/${encodeURIComponent(jobName)}/build`;

      const response = await this.client.post(endpoint, hasParameters ? parameters : null);

      const queueLocation = response.headers.location;
      const queueId = queueLocation ? queueLocation.split('/').pop() : null;

      logger.info(`Build triggered for job ${jobName}`, { queueId, parameters });

      return {
        jobName,
        queueId,
        parameters,
        message: 'ë¹Œë“œê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.',
      };
    } catch (error) {
      logger.error(`Failed to trigger build for job ${jobName}:`, error.message);
      throw new Error(`Jenkins ì‘ì—… ${jobName} ë¹Œë“œë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
    }
  }

  async stopBuild(jobName, buildNumber) {
    try {
      // projects í´ë” í•˜ìœ„ì˜ ì‘ì—… ë¹Œë“œ ì¤‘ì§€
      await this.client.post(`/job/projects/job/${encodeURIComponent(jobName)}/${buildNumber}/stop`);

      logger.info(`Build stopped for job ${jobName}#${buildNumber}`);

      return {
        jobName,
        buildNumber,
        message: 'ë¹Œë“œê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.',
      };
    } catch (error) {
      logger.error(`Failed to stop build ${jobName}#${buildNumber}:`, error.message);
      throw new Error(`Jenkins ë¹Œë“œ ${jobName}#${buildNumber}ë¥¼ ì¤‘ì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.`);
    }
  }

  async getQueueInfo(queueId) {
    try {
      const response = await this.client.get('/queue/api/json?tree=items[id,task[name],stuck,blocked,buildable,params,why,inQueueSince]');
      const queueItem = response.data.items.find(item => item.id === parseInt(queueId));

      if (queueItem) {
        return {
          id: queueItem.id,
          jobName: queueItem.task.name,
          stuck: queueItem.stuck,
          blocked: queueItem.blocked,
          buildable: queueItem.buildable,
          reason: queueItem.why,
          inQueueSince: new Date(queueItem.inQueueSince),
        };
      }

      return null;
    } catch (error) {
      logger.error(`Failed to fetch queue info for ${queueId}:`, error.message);
      return null;
    }
  }

  async getRecentBuilds(hours = null, limit = 50) {
    try {
      logger.debug(`getRecentBuilds called with hours=${hours}, limit=${limit}`);
      const jobs = await this.getJobs();
      logger.debug(`Retrieved ${jobs.length} jobs for recent builds`);
      const allBuilds = [];

      // hoursê°€ nullì¸ ê²½ìš° ì‹œê°„ ì œí•œ ì—†ìŒ
      let cutoffTime = null;
      if (hours !== null && hours !== undefined) {
        cutoffTime = new Date(Date.now() - (hours * 60 * 60 * 1000));
        logger.debug(`Cutoff time for recent builds: ${cutoffTime.toISOString()}`);
      } else {
        logger.debug('No time limit set - fetching all builds');
      }

      for (const job of jobs) {
        try {
          logger.debug(`Fetching builds for job: ${job.fullJobName}`);
          const builds = await this.getJobBuilds(job.fullJobName, 100);
          logger.debug(`Retrieved ${builds.length} builds for job ${job.fullJobName}`);

          if (builds.length === 0) {
            // ë¹Œë“œê°€ ì—†ëŠ” ê²½ìš° í”„ë¡œì íŠ¸ ê¸°ë³¸ ì •ë³´ ìƒì„±
            const projectEntry = {
              id: `${job.fullJobName}-no-builds`,
              projectName: job.fullJobName,
              buildNumber: null,
              status: 'no_builds',
              timestamp: new Date(),
              duration: null,
              displayName: `${job.fullJobName} (ë¹Œë“œ ì—†ìŒ)`,
              url: job.url,
              parameters: {},
              changes: [],
            };
            allBuilds.push(projectEntry);
          } else {
            const recentBuilds = cutoffTime
              ? builds.filter(build => build.timestamp >= cutoffTime)
              : builds; // cutoffTimeì´ nullì´ë©´ ëª¨ë“  ë¹Œë“œ í¬í•¨
            logger.debug(`Filtered to ${recentBuilds.length} recent builds for job ${job.fullJobName}`);
            allBuilds.push(...recentBuilds);
          }
        } catch (error) {
          logger.error(`Failed to fetch builds for job ${job.fullJobName}:`, error.message, error.stack);

          // ì—ëŸ¬ê°€ ë°œìƒí•œ ê²½ìš°ì—ë„ í”„ë¡œì íŠ¸ ì •ë³´ í‘œì‹œ
          const errorEntry = {
            id: `${job.fullJobName}-error`,
            projectName: job.fullJobName,
            buildNumber: null,
            status: 'error',
            timestamp: new Date(),
            duration: null,
            displayName: `${job.fullJobName} (ì˜¤ë¥˜)`,
            url: job.url,
            parameters: {},
            changes: [],
          };
          allBuilds.push(errorEntry);
        }
      }

      logger.debug(`Total builds collected: ${allBuilds.length}`);
      allBuilds.sort((a, b) => new Date(b.timestamp) - new Date(a.timestamp));

      const result = allBuilds.slice(0, limit);
      logger.debug(`Returning ${result.length} recent builds after limit`);
      return result;
    } catch (error) {
      logger.error('Failed to fetch recent builds:', error.message, error.stack);
      throw new Error('ìµœê·¼ ë¹Œë“œ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
    }
  }

  mapJenkinsStatus(result) {
    if (result === null) return 'in_progress';

    switch (result) {
      case 'SUCCESS': return 'success';
      case 'FAILURE': return 'failed';
      case 'ABORTED': return 'cancelled';
      case 'UNSTABLE': return 'failed';
      default: return 'unknown';
    }
  }

  extractParameters(actions) {
    const parameters = {};
    if (actions) {
      for (const action of actions) {
        if (action.parameters) {
          for (const param of action.parameters) {
            parameters[param.name] = param.value;
          }
        }
      }
    }
    return parameters;
  }

  extractChanges(changeSet) {
    if (!changeSet || !changeSet.items) return [];

    return changeSet.items.map(item => ({
      commitId: item.commitId,
      message: item.msg,
      author: item.author ? item.author.fullName : 'Unknown',
    }));
  }

  extractBranchInfo(actions) {
    if (!actions || !Array.isArray(actions)) return 'main';


    // Git ë¸Œëœì¹˜ ì •ë³´ ì°¾ê¸°
    for (const action of actions) {

      // Git pluginì˜ lastBuiltRevisionì—ì„œ ë¸Œëœì¹˜ ì •ë³´ ì¶”ì¶œ
      if (action.lastBuiltRevision && action.lastBuiltRevision.branch) {
        const branches = action.lastBuiltRevision.branch;
        if (Array.isArray(branches) && branches.length > 0) {
          const branchName = branches[0].name;
          // origin/ ì ‘ë‘ì‚¬ ì œê±°
          return branchName ? branchName.replace('origin/', '').replace('refs/heads/', '') : 'main';
        }
      }

      // hudson.plugins.git.util.BuildData í´ë˜ìŠ¤ì—ì„œ ë¸Œëœì¹˜ ì •ë³´ ì°¾ê¸°
      if (action._class === 'hudson.plugins.git.util.BuildData' && action.lastBuiltRevision) {
        if (action.lastBuiltRevision.branch && Array.isArray(action.lastBuiltRevision.branch)) {
          const branchName = action.lastBuiltRevision.branch[0]?.name;
          if (branchName) {
            return branchName.replace('origin/', '').replace('refs/heads/', '');
          }
        }
      }

      // parametersì—ì„œ ë¸Œëœì¹˜ ì •ë³´ ì°¾ê¸° (parametrized buildì¸ ê²½ìš°)
      if (action.parameters && Array.isArray(action.parameters)) {
        const branchParam = action.parameters.find(param =>
          param.name && (
            param.name.toLowerCase() === 'branch' ||
            param.name.toLowerCase() === 'git_branch' ||
            param.name.toLowerCase() === 'branch_name'
          ),
        );
        if (branchParam && branchParam.value) {
          return branchParam.value.replace('refs/heads/', '').replace('origin/', '');
        }
      }
    }

    return 'main'; // ê¸°ë³¸ê°’
  }

  detectLogLevel(line) {
    const upperLine = line.toUpperCase();
    if (upperLine.includes('ERROR') || upperLine.includes('FAILED')) return 'ERROR';
    if (upperLine.includes('WARN')) return 'WARN';
    if (upperLine.includes('SUCCESS') || upperLine.includes('FINISHED')) return 'SUCCESS';
    return 'INFO';
  }

  parseJenkinsConsoleLog(logContent, jobName, buildNumber) {
    const lines = logContent.split('\n');
    const logs = [];
    let buildStartTime = null;

    for (let i = 0; i < lines.length; i++) {
      const line = lines[i].trim();
      if (!line) continue;

      // Jenkins íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ ê°ì§€ (ì˜ˆ: [2025-09-29T12:30:45.123Z])
      const timestampMatch = line.match(/^\[?(\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}(?:\.\d{3})?[Z]?)\]?/);
      let timestamp;

      if (timestampMatch) {
        timestamp = timestampMatch[1];
      } else if (buildStartTime) {
        // ë¹Œë“œ ì‹œì‘ ì‹œê°„ì„ ê¸°ì¤€ìœ¼ë¡œ ì¶”ì •
        timestamp = new Date(new Date(buildStartTime).getTime() + (i * 1000)).toISOString();
      } else {
        // í˜„ì¬ ì‹œê°„ì—ì„œ ì¶”ì •
        timestamp = new Date(Date.now() - (lines.length - i) * 1000).toISOString();
      }

      // ë¹Œë“œ ì‹œì‘ ì‹œê°„ ê°ì§€
      if (line.includes('Started by') || line.includes('Building in workspace')) {
        buildStartTime = timestamp;
      }

      // ë¡œê·¸ ë©”ì‹œì§€ì—ì„œ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê±°
      const message = timestampMatch ? line.replace(timestampMatch[0], '').trim() : line;

      // Jenkins ì½˜ì†”ì—ì„œ ì¤‘ìš”í•œ ë‹¨ê³„ë§Œ í•„í„°ë§
      if (this.isImportantLogLine(line)) {
        logs.push({
          timestamp: this.formatTimestamp(timestamp),
          level: this.detectLogLevel(line),
          message: message || line,
          lineNumber: i + 1,
          jobName: jobName,
          buildNumber: buildNumber,
        });
      }
    }

    // ë¡œê·¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ëª¨ë“  ë¼ì¸ì„ í¬í•¨
    if (logs.length < 5) {
      return lines.map((line, index) => ({
        timestamp: this.formatTimestamp(new Date(Date.now() - (lines.length - index) * 1000).toISOString()),
        level: this.detectLogLevel(line),
        message: line.trim() || line,
        lineNumber: index + 1,
        jobName: jobName,
        buildNumber: buildNumber,
      })).filter(log => log.message);
    }

    return logs;
  }

  isImportantLogLine(line) {
    const importantPatterns = [
      /Started by/i,
      /Building in workspace/i,
      /Checkout/i,
      /Building/i,
      /Compiling/i,
      /Testing/i,
      /Deploying/i,
      /Publishing/i,
      /Archiving/i,
      /SUCCESS/i,
      /FAILURE/i,
      /ERROR/i,
      /WARNING/i,
      /Finished:/i,
      /\+ /,  // Shell commands
      />\s*.*\.sh/i,  // Script execution
      /maven/i,
      /gradle/i,
      /npm/i,
      /docker/i,
      /kubectl/i,
      /git/i,
      /tar/i,
      /deploy/i,
      /release/i,
      /version/i,
    ];

    return importantPatterns.some(pattern => pattern.test(line));
  }

  formatTimestamp(timestamp) {
    try {
      return new Date(timestamp).toLocaleString('ko-KR', {
        year: 'numeric',
        month: '2-digit',
        day: '2-digit',
        hour: '2-digit',
        minute: '2-digit',
        second: '2-digit',
        hour12: false,
      });
    } catch (error) {
      return timestamp;
    }
  }

  // Alias for backward compatibility
  async getRecentDeployments(hours = null, limit = 50) {
    return await this.getRecentBuilds(hours, limit);
  }

  async getSystemInfo() {
    return await this.healthCheck();
  }

  async healthCheck() {
    try {
      const response = await this.client.get('/api/json');
      return {
        status: 'healthy',
        version: response.data.version || 'unknown',
        mode: response.data.mode || 'unknown',
        nodeDescription: response.data.nodeDescription || 'unknown',
      };
    } catch (error) {
      logger.error('Jenkins health check failed:', error.message);
      return {
        status: 'unhealthy',
        error: error.message,
      };
    }
  }
}

let jenkinsService = null;

function getJenkinsService() {
  if (!jenkinsService) {
    jenkinsService = new JenkinsService();
  }
  return jenkinsService;
}

module.exports = {
  JenkinsService,
  getJenkinsService,
};
